# scripts/query_rag.py
import os
import streamlit as st
from pathlib import Path
import json
# å…¨éƒ¨ä¿®æ­£åçš„ imports
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import CTransformers
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA


EMBED_MODEL = "BAAI/bge-large-zh" # "BAAI/bge-large-zh" "BAAI/bge-small-zh"

BASE_DIR = Path(__file__).resolve().parent.parent
CHAT_HISTORY_FILE = BASE_DIR / "chat_history.json"
# ä¸­æ–‡æç¤ºæ¨¡æ¿
PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„è¿ç»´ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹çŸ¥è¯†å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
è‹¥æ— æ³•ç›´æ¥ä»ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·åŸºäºå¸¸è¯†åˆç†æ¨æ–­ã€‚

çŸ¥è¯†å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·ç”¨æ¸…æ™°ç®€æ´çš„ä¸­æ–‡å›ç­”ã€‚
"""

# prompt = PromptTemplate(
#     template=PROMPT_TEMPLATE,
#     input_variables=["context", "question"]
# )

MODEL_LIST = {
    "llama-2-7b.Q2_K": {
        "path": str(BASE_DIR / "models" / "llama" / "llama-2-7b.Q2_K.gguf"),
        "model_type": "llama"
    },
    "MiniCPM-2B-128k-Q2_K": {
        "path": str(BASE_DIR / "models" / "minicpm" / "MiniCPM-2B-128k-Q2_K.gguf"),
        "model_type": "llama"
    },
    "minicpm-2b-dpo-fp32.Q5_K_M": {
        "path": str(BASE_DIR / "models" / "minicpm" / "minicpm-2b-dpo-fp32.Q5_K_M.gguf"),
        "model_type": "llama"
    }
}


@st.cache_resource
def load_vector_store():
    model = HuggingFaceEmbeddings(model_name=EMBED_MODEL) # "BAAI/bge-small-zh"
    return FAISS.load_local(
        "embeddings/faiss_store",
        model,
        allow_dangerous_deserialization=True
    )
@st.cache_resource
def load_llm(model_cfg):
    return CTransformers(
        model=model_cfg["path"],
        model_type=model_cfg["model_type"],
        config={"max_new_tokens":512, "temperature":0.7,
                # "gpu_layers": 10,
                # "context_length": 2048
                }
    )

def load_chat_history():
    if CHAT_HISTORY_FILE.exists():
        with open(CHAT_HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_chat_history(history):
    with open(CHAT_HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def main():
    st.set_page_config(page_title="ä¸­æ–‡RAGé—®ç­”", layout="wide")
    st.title("ğŸ“˜ æœ¬åœ°ä¸­æ–‡è¿ç»´æ™ºèƒ½é—®ç­”")
    db = load_vector_store()
    prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=["context","question"])
    history = load_chat_history()

    with st.sidebar:
        model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", list(MODEL_LIST.keys()))
        query = st.text_area("è¾“å…¥ä½ çš„é—®é¢˜", "", height=120)
        do = st.button("ğŸ” æé—®")
        if st.button("ğŸ—‘ æ¸…ç©ºå†å²"):
            history = []
            save_chat_history(history)
            st.rerun()


    if do and query:
        with st.spinner("å¤„ç†ä¸­..."):
            llm = load_llm(MODEL_LIST[model_name])
            retriever = db.as_retriever(search_type="similarity", search_kwargs={"k":8})
            qa = RetrievalQA.from_chain_type(
                llm=llm, chain_type="stuff", retriever=retriever,
                return_source_documents=True, chain_type_kwargs={"prompt":prompt}
            )
            res = qa.invoke(query)
            answer = res["result"]

            history.append({
                "question": query,
                "answer": answer,
                "sources": [
                    {"source": doc.metadata.get("source", ""), "content": doc.page_content[:300]+"..."}
                    for doc in res["source_documents"]
                ]
            })
            save_chat_history(history)
            st.rerun()


    st.subheader("ğŸ’¬ å†å²é—®ç­”")
    if history:
        for idx, chat in enumerate(reversed(history), 1):
            st.markdown(f"**{idx}. ç”¨æˆ·é—®é¢˜:** {chat['question']}")
            st.markdown(f"**ğŸ¤– å›ç­”:** {chat['answer']}")
            if chat["sources"]:
                with st.expander("æŸ¥çœ‹å‚è€ƒç‰‡æ®µ"):
                    for sidx, s in enumerate(chat["sources"], 1):
                        st.markdown(f"**ç‰‡æ®µ{sidx}: {s['source']}**")
                        st.write(s["content"])
    else:
        st.info("æš‚æ— å†å²è®°å½•ã€‚")

if __name__ == '__main__':
    main()
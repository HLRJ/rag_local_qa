# scripts/build_vector_stroe.py
from pathlib import Path
import os
import json
from langchain_community.document_loaders import (
    PyMuPDFLoader, UnstructuredWordDocumentLoader, UnstructuredExcelLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½• scripts/
SCRIPT_DIR = Path(__file__).resolve().parent

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = SCRIPT_DIR.parent

# æ­£ç¡®çš„ data è·¯å¾„
SOURCE_DIR = PROJECT_ROOT / "data"

EMBEDDING_DIR = PROJECT_ROOT / "embeddings/faiss_store"
RECORD_FILE = EMBEDDING_DIR / "record.json"
EMBED_MODEL = "BAAI/bge-large-zh"
# EMBED_MODEL = "BAAI/bge-small-zh"

def load_indexed_files():
    if not RECORD_FILE.exists():
        return []
    with open(RECORD_FILE, "r", encoding="utf-8") as f:
        return json.load(f).get("indexed_files", [])

def save_indexed_files(file_list):
    with open(RECORD_FILE, "w", encoding="utf-8") as f:
        json.dump({"indexed_files": file_list}, f, ensure_ascii=False, indent=2)

def load_new_documents(indexed_files):
    new_docs = []
    new_files = []

    files = list(SOURCE_DIR.glob("*"))
    print("ğŸ“ å½“å‰ data æ–‡ä»¶å¤¹å†…å®¹:", [str(f) for f in files])
    print("ç»å¯¹è·¯å¾„:", SOURCE_DIR.resolve())

    for f in files:
        print("æ­£åœ¨å¤„ç†æ–‡ä»¶:", f)
        fname = f.name

        if fname in indexed_files:
            print(f"âœ… æ–‡ä»¶å·²å­˜åœ¨ç´¢å¼•ï¼Œè·³è¿‡: {fname}")
            continue

        loader = None
        try:
            if f.suffix.lower() == ".pdf":
                loader = PyMuPDFLoader(str(f))
            elif f.suffix.lower() == ".docx" : # or f.suffix.lower() == ".doc"
                loader = UnstructuredWordDocumentLoader(str(f))
            elif f.suffix.lower() == ".xlsx" : # or f.suffix.lower() == "xls"
                loader = UnstructuredExcelLoader(str(f))

            if loader:
                docs = loader.load()
                for doc in docs:
                    doc.metadata["source"] = fname
                    new_docs.append(doc)
                new_files.append(fname)
                print(f"âœ… æˆåŠŸåŠ è½½æ–‡æ¡£ {fname}ï¼Œå…± {len(docs)} æ®µæ–‡æœ¬ã€‚")
            else:
                print(f"âš ï¸ æ–‡ä»¶ {fname} æ ¼å¼ä¸è¢«æ”¯æŒï¼Œå·²è·³è¿‡ã€‚")

        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶ {fname} å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")

    return new_docs, new_files

def main():
    os.makedirs(EMBEDDING_DIR, exist_ok=True)
    indexed_files = load_indexed_files()
    print(f"âœ… å·²ç´¢å¼•æ–‡ä»¶: {indexed_files}")

    new_docs, new_files = load_new_documents(indexed_files)
    if not new_docs:
        print("âœ… æ²¡æœ‰å‘ç°æ–°çš„æ–‡ä»¶éœ€è¦ç´¢å¼•ï¼Œå·²è·³è¿‡ã€‚")
        return

    print(f"ğŸ“„ æœ¬æ¬¡æ–°å¢ {len(new_files)} æ–‡ä»¶: {new_files}")
    # splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=200
    )
    new_chunks = splitter.split_documents(new_docs)
    print(f"âœ‚ï¸ åˆ‡åˆ†åå¾—åˆ° {len(new_chunks)} ä¸ªæ–‡æœ¬å—")

    embed = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
    db_new = FAISS.from_documents(new_chunks, embed)

    if (EMBEDDING_DIR / "index.faiss").exists():
        print("ğŸ”— åŠ è½½å·²æœ‰å‘é‡åº“å¹¶åˆå¹¶...")
        db_existing = FAISS.load_local(str(EMBEDDING_DIR), embed, allow_dangerous_deserialization=True)
        db_existing.merge_from(db_new)
        db_existing.save_local(str(EMBEDDING_DIR))
    else:
        print("ğŸš€ é¦–æ¬¡ç”Ÿæˆå‘é‡åº“...")
        db_new.save_local(str(EMBEDDING_DIR))

    indexed_files += new_files
    save_indexed_files(indexed_files)
    print("ğŸ‰ æœ¬æ¬¡æ–°å¢çŸ¥è¯†å·²ç´¢å¼•å®Œæˆï¼")

if __name__ == "__main__":
    main()

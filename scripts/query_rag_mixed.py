# æ–‡ä»¶ï¼šscripts/query_rag_mixed.py
import os
import json
from pathlib import Path
import streamlit as st
import torch
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import CTransformers
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline


# ========== åŸºç¡€è·¯å¾„ ==========
BASE_DIR = Path(__file__).resolve().parent.parent
EMBEDDING_PATH = BASE_DIR / "embeddings/faiss_store"
CHAT_HISTORY_FILE = BASE_DIR / "chat_history.json"

# ========== æ¨¡å‹åˆ—è¡¨ ==========
MODEL_CONFIG = {
    "llama-2-7b.Q2_K": {
        "type": "gguf",
        "model_path": str(BASE_DIR / "models" / "llama" / "llama-2-7b.Q2_K.gguf"),
        "model_type": "llama"
    },
    "Qwen-1.8B-SAFETENSORS": {
        "type": "hf",
        "model_path": BASE_DIR / "models/Qwen/Qwen1.5-1.8B",  # huggingfaceè·¯å¾„æˆ–æœ¬åœ°è·¯å¾„
    }
}

# ========== PROMPT æ¨¡æ¿ ==========
PROMPT_TEMPLATE = """
å·²çŸ¥ä¿¡æ¯å¦‚ä¸‹ï¼š
----------------
{context}
----------------

ç”¨æˆ·æé—®ï¼š
{question}

### å›ç­”ï¼š
"""


# ========== å‘é‡åº“åŠ è½½ ==========
@st.cache_resource
def load_vector_store():
    embed = HuggingFaceEmbeddings(model_name="BAAI/bge-large-zh")
    return FAISS.load_local(str(EMBEDDING_PATH), embed, allow_dangerous_deserialization=True)

# ========== æ¨¡å‹åŠ è½½é€»è¾‘ ==========
@st.cache_resource
def load_llm(model_key):
    config = MODEL_CONFIG[model_key]
    if config["type"] == "gguf":
        return CTransformers(
            model=str(config["model_path"]),
            model_type=config["model_type"],
            config={"max_new_tokens": 512, "temperature": 0.7}
        )
    elif config["type"] == "hf":
        tokenizer = AutoTokenizer.from_pretrained(config["model_path"], trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            config["model_path"],
            device_map="auto",
            torch_dtype=torch.float16,
            load_in_4bit=True  # è‡ªåŠ¨é€‚é…ä½æ˜¾å­˜
        )
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
            temperature=0.7,
            repetition_penalty=1.1
        )
        return HuggingFacePipeline(pipeline=pipe)
    else:
        raise ValueError("ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹")

# ========== å†å²é—®ç­” ==========
def load_chat_history():
    if CHAT_HISTORY_FILE.exists():
        with open(CHAT_HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_chat_history(history):
    with open(CHAT_HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

# ========== ä¸»ç•Œé¢ ==========
def main():
    st.set_page_config(page_title="RAG è¿ç»´é—®ç­”ï¼ˆGGUF + safetensorsï¼‰", layout="wide")
    st.title("ğŸ“˜ æœ¬åœ°ä¸­æ–‡è¿ç»´æ™ºèƒ½é—®ç­”")

    db = load_vector_store()
    prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=["context", "question"])
    history = load_chat_history()

    with st.sidebar:
        model_choice = st.selectbox("é€‰æ‹©æ¨¡å‹ï¼š", list(MODEL_CONFIG.keys()))
        query = st.text_area("è¾“å…¥ä½ çš„é—®é¢˜ï¼š", "", height=150)
        do = st.button("ğŸ” æé—®")
        if st.button("ğŸ§¹ æ¸…ç©ºå†å²è®°å½•"):
            history = []
            save_chat_history(history)
            st.rerun()

    if do and query.strip():
        with st.spinner("ğŸ”„ æ­£åœ¨å¤„ç†..."):
            llm = load_llm(model_choice)
            retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 8, "score_threshold": 0.3})
            qa = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True,
                chain_type_kwargs={"prompt": prompt}
            )
            res = qa.invoke(query)
            answer = res["result"]

            history.append({
                "question": query,
                "answer": answer,
                "sources": [
                    {"source": doc.metadata.get("source", ""), "content": doc.page_content[:300] + "..."}
                    for doc in res["source_documents"]
                ]
            })
            save_chat_history(history)
            st.rerun()

    st.subheader("ğŸ’¬ å†å²é—®ç­”è®°å½•")
    if history:
        for idx, chat in enumerate(reversed(history), 1):
            st.markdown(f"**{idx}. ç”¨æˆ·æé—®ï¼š** {chat['question']}")
            st.markdown(f"**ğŸ¤– å›ç­”ï¼š** {chat['answer']}")
            if chat["sources"]:
                with st.expander("ğŸ“„ æŸ¥çœ‹å‚è€ƒç‰‡æ®µ"):
                    for i, s in enumerate(chat["sources"], 1):
                        st.markdown(f"**ç‰‡æ®µ{i}ï¼š{s['source']}**")
                        st.write(s["content"])
    else:
        st.info("æš‚æ— å†å²è®°å½•")

if __name__ == "__main__":
    main()
